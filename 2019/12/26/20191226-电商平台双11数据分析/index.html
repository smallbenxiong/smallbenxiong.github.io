<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Hadoop | Hadoop双11电商平台数据分析 | Ben Blog</title><meta name="description" content="Hadoop双11电商平台数据分析"><meta name="keywords" content="Hadoop,Hive,Spark,数据分析"><meta name="author" content="Ben"><meta name="copyright" content="Ben"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="msvalidate.01" content="rZnjFqvEYD"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Hadoop | Hadoop双11电商平台数据分析"><meta name="twitter:description" content="Hadoop双11电商平台数据分析"><meta name="twitter:image" content="https://img.alicdn.com/tfs/TB1MaLKRXXXXXaWXFXXXXXXXXXX-480-260.png"><meta property="og:type" content="article"><meta property="og:title" content="Hadoop | Hadoop双11电商平台数据分析"><meta property="og:url" content="https://smallbenxiong.github.io/2019/12/26/20191226-%E7%94%B5%E5%95%86%E5%B9%B3%E5%8F%B0%E5%8F%8C11%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"><meta property="og:site_name" content="Ben Blog"><meta property="og:description" content="Hadoop双11电商平台数据分析"><meta property="og:image" content="https://img.alicdn.com/tfs/TB1MaLKRXXXXXaWXFXXXXXXXXXX-480-260.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.15/dist/snackbar.min.css"><link rel="canonical" href="https://smallbenxiong.github.io/2019/12/26/20191226-%E7%94%B5%E5%95%86%E5%B9%B3%E5%8F%B0%E5%8F%8C11%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"><link rel="prev" title="TensorFlow | TensorFlow实现单变量线性回归" href="https://smallbenxiong.github.io/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><link rel="next" title="Sqoop | Sqoop报错解决" href="https://smallbenxiong.github.io/2019/12/20/20191220-Sqoop%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"JUCF6SIZ0I","apiKey":"704198e89a5f8a6d8fffde9b29c9b183","indexName":"smallbenxiong_hexo","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: {"languages":{"author":"作者: Ben","link":"链接: https://smallbenxiong.github.io/2019/12/26/20191226-%E7%94%B5%E5%95%86%E5%B9%B3%E5%8F%B0%E5%8F%8C11%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","source":"来源: Ben Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  copy_copyright_js: true,
  ClickShowText: undefined,
  medium_zoom: 'true',
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-right"}
  
}</script><meta name="generator" content="Hexo 4.1.1"><link rel="alternate" href="/atom.xml" title="Ben Blog" type="application/atom+xml">
</head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Ben Blog</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/projects/projects.html"><i class="fa-fw fa fa-file"></i><span> Project</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://avatars3.githubusercontent.com/u/33283932?s=460&amp;v=4" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">8</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">20</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">6</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/projects/projects.html"><i class="fa-fw fa fa-file"></i><span> Project</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#一、实验平台"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">一、实验平台</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Centos"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">Centos</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1、登录"><span class="toc_mobile_items-number">1.1.1.</span> <span class="toc_mobile_items-text">1、登录</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2、修改ip地址"><span class="toc_mobile_items-number">1.1.2.</span> <span class="toc_mobile_items-text">2、修改ip地址</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3、虚拟机和电脑可ping可通"><span class="toc_mobile_items-number">1.1.3.</span> <span class="toc_mobile_items-text">3、虚拟机和电脑可ping可通</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4、启动Hadoop平台"><span class="toc_mobile_items-number">1.1.4.</span> <span class="toc_mobile_items-text">4、启动Hadoop平台</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Windows"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">Windows</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1、版本"><span class="toc_mobile_items-number">1.2.1.</span> <span class="toc_mobile_items-text">1、版本</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2、Hosts"><span class="toc_mobile_items-number">1.2.2.</span> <span class="toc_mobile_items-text">2、Hosts</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#二、实验数据"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">二、实验数据</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1、数据集"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">1、数据集</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2、数据上传"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">2、数据上传</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1、HDFS创建文件夹-本次实验忽略"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text">1、HDFS创建文件夹(本次实验忽略)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2、上传数据至Centos"><span class="toc_mobile_items-number">2.2.2.</span> <span class="toc_mobile_items-text">2、上传数据至Centos</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3、上传数据至HDFS-本次实验忽略"><span class="toc_mobile_items-number">2.2.3.</span> <span class="toc_mobile_items-text">3、上传数据至HDFS(本次实验忽略)</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#3、解压文件"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">3、解压文件</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#4、数据预处理"><span class="toc_mobile_items-number">2.4.</span> <span class="toc_mobile_items-text">4、数据预处理</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1、删除标签数据"><span class="toc_mobile_items-number">2.4.1.</span> <span class="toc_mobile_items-text">1、删除标签数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2、查看文件行数"><span class="toc_mobile_items-number">2.4.2.</span> <span class="toc_mobile_items-text">2、查看文件行数</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3、数据截取"><span class="toc_mobile_items-number">2.4.3.</span> <span class="toc_mobile_items-text">3、数据截取</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#三、Hive实验"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">三、Hive实验</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#平台启动"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">平台启动</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#MySQL启动"><span class="toc_mobile_items-number">3.1.1.</span> <span class="toc_mobile_items-text">MySQL启动</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Hive启动"><span class="toc_mobile_items-number">3.1.2.</span> <span class="toc_mobile_items-text">Hive启动</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据上传"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">数据上传</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据查询"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">数据查询</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#查询数据插入临时表"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text">查询数据插入临时表</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Sqoop导入MySQL"><span class="toc_mobile_items-number">3.5.</span> <span class="toc_mobile_items-text">Sqoop导入MySQL</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#错误"><span class="toc_mobile_items-number">3.6.</span> <span class="toc_mobile_items-text">错误</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#四、Spark"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">四、Spark</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、实验平台"><span class="toc-number">1.</span> <span class="toc-text">一、实验平台</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Centos"><span class="toc-number">1.1.</span> <span class="toc-text">Centos</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、登录"><span class="toc-number">1.1.1.</span> <span class="toc-text">1、登录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、修改ip地址"><span class="toc-number">1.1.2.</span> <span class="toc-text">2、修改ip地址</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、虚拟机和电脑可ping可通"><span class="toc-number">1.1.3.</span> <span class="toc-text">3、虚拟机和电脑可ping可通</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、启动Hadoop平台"><span class="toc-number">1.1.4.</span> <span class="toc-text">4、启动Hadoop平台</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Windows"><span class="toc-number">1.2.</span> <span class="toc-text">Windows</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、版本"><span class="toc-number">1.2.1.</span> <span class="toc-text">1、版本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、Hosts"><span class="toc-number">1.2.2.</span> <span class="toc-text">2、Hosts</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、实验数据"><span class="toc-number">2.</span> <span class="toc-text">二、实验数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、数据集"><span class="toc-number">2.1.</span> <span class="toc-text">1、数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、数据上传"><span class="toc-number">2.2.</span> <span class="toc-text">2、数据上传</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、HDFS创建文件夹-本次实验忽略"><span class="toc-number">2.2.1.</span> <span class="toc-text">1、HDFS创建文件夹(本次实验忽略)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、上传数据至Centos"><span class="toc-number">2.2.2.</span> <span class="toc-text">2、上传数据至Centos</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、上传数据至HDFS-本次实验忽略"><span class="toc-number">2.2.3.</span> <span class="toc-text">3、上传数据至HDFS(本次实验忽略)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、解压文件"><span class="toc-number">2.3.</span> <span class="toc-text">3、解压文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、数据预处理"><span class="toc-number">2.4.</span> <span class="toc-text">4、数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、删除标签数据"><span class="toc-number">2.4.1.</span> <span class="toc-text">1、删除标签数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、查看文件行数"><span class="toc-number">2.4.2.</span> <span class="toc-text">2、查看文件行数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、数据截取"><span class="toc-number">2.4.3.</span> <span class="toc-text">3、数据截取</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三、Hive实验"><span class="toc-number">3.</span> <span class="toc-text">三、Hive实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#平台启动"><span class="toc-number">3.1.</span> <span class="toc-text">平台启动</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MySQL启动"><span class="toc-number">3.1.1.</span> <span class="toc-text">MySQL启动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive启动"><span class="toc-number">3.1.2.</span> <span class="toc-text">Hive启动</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据上传"><span class="toc-number">3.2.</span> <span class="toc-text">数据上传</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据查询"><span class="toc-number">3.3.</span> <span class="toc-text">数据查询</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#查询数据插入临时表"><span class="toc-number">3.4.</span> <span class="toc-text">查询数据插入临时表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sqoop导入MySQL"><span class="toc-number">3.5.</span> <span class="toc-text">Sqoop导入MySQL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#错误"><span class="toc-number">3.6.</span> <span class="toc-text">错误</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#四、Spark"><span class="toc-number">4.</span> <span class="toc-text">四、Spark</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img.alicdn.com/tfs/TB1MaLKRXXXXXaWXFXXXXXXXXXX-480-260.png)"><div id="post-info"><div id="post-title"><div class="posttitle">Hadoop | Hadoop双11电商平台数据分析</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-12-26<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-04-27</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Hadoop/">Hadoop</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon" aria-hidden="true"></i><span>字数总计: </span><span class="word-count">4.4k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon" aria-hidden="true"></i><span>阅读时长: 23 分钟</span><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="一、实验平台"><a href="#一、实验平台" class="headerlink" title="一、实验平台"></a>一、实验平台</h1><h2 id="Centos"><a href="#Centos" class="headerlink" title="Centos"></a>Centos</h2><h3 id="1、登录"><a href="#1、登录" class="headerlink" title="1、登录"></a>1、登录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">平台账号密码</span><br><span class="line">$ hadoop&#x2F;hadoop</span><br><span class="line">$ root&#x2F;hadoop01</span><br></pre></td></tr></table></figure>
<h3 id="2、修改ip地址"><a href="#2、修改ip地址" class="headerlink" title="2、修改ip地址"></a>2、修改ip地址</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-enp0s3</span><br><span class="line">  IPADDR&#x3D;  192.168.43.73</span><br><span class="line">  NETMASK&#x3D;  255.255.255.0</span><br><span class="line">  GATEWAY&#x3D; 192.168.43.1</span><br><span class="line"># vi &#x2F;etc&#x2F;hosts</span><br><span class="line">$ 添加一行 </span><br><span class="line">  192.168.43.73 bp01</span><br><span class="line">$ 重启服务</span><br><span class="line"># service network restart</span><br></pre></td></tr></table></figure>
<h3 id="3、虚拟机和电脑可ping可通"><a href="#3、虚拟机和电脑可ping可通" class="headerlink" title="3、虚拟机和电脑可ping可通"></a>3、虚拟机和电脑可ping可通</h3><p>*** 主机可ping虚拟机，虚拟机ping不了主机的解决方法：win10防火墙 → 高级设置 → 入站规则，找到配置文件类型为“公用”的“文件和打印共享（回显请求 – ICMPv4-In）”规则，设置为允许即可。</p>
<h3 id="4、启动Hadoop平台"><a href="#4、启动Hadoop平台" class="headerlink" title="4、启动Hadoop平台"></a>4、启动Hadoop平台</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ 启动Hadoop平台</span><br><span class="line"># start-all.sh</span><br></pre></td></tr></table></figure>
<p>浏览器可访问 <a href="http://192.168.43.73:50070/" target="_blank" rel="noopener">http://192.168.43.73:50070/</a> 和 <a href="http://192.168.43.73:8088/" target="_blank" rel="noopener">http://192.168.43.73:8088/</a> 即启动成功。</p>
<h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><h3 id="1、版本"><a href="#1、版本" class="headerlink" title="1、版本"></a>1、版本</h3><p>Windows7、Jdk 1.8</p>
<h3 id="2、Hosts"><a href="#2、Hosts" class="headerlink" title="2、Hosts"></a>2、Hosts</h3><p>C:/Windows/System32/drivers/etc/</p>
<h1 id="二、实验数据"><a href="#二、实验数据" class="headerlink" title="二、实验数据"></a>二、实验数据</h1><h2 id="1、数据集"><a href="#1、数据集" class="headerlink" title="1、数据集"></a>1、数据集</h2><p>三个文件：用户行为日志文件user_log.csv、回头客训练集train.csv、回头客测试集test.csv。</p>
<p>1、 用户行为日志user_log.csv，日志中的字段定义如下：</p>
<ol>
<li>user_id | 买家id</li>
<li>item_id | 商品id</li>
<li>cat_id | 商品类别id</li>
<li>merchant_id | 卖家id</li>
<li>brand_id | 品牌id</li>
<li>month | 交易时间:月</li>
<li>day | 交易事件:日</li>
<li>action | 行为,取值范围{0,1,2,3},0表示点击，1表示加入购物车，2表示购买，3表示关注商品</li>
<li>age_range | 买家年龄分段：1表示年龄&lt;18,2表示年龄在[18,24]，3表示年龄在[25,29]，4表示年龄在[30,34]，5表示年龄在[35,39]，6表示年龄在[40,49]，7和8表示年龄&gt;=50,0和NULL则表示未知</li>
<li>gender | 性别:0表示女性，1表示男性，2和NULL表示未知</li>
<li>province| 收货地址省份   </li>
</ol>
<p>2、 回头客训练集train.csv和回头客测试集test.csv，字段定义如下：</p>
<ol>
<li>user_id | 买家id</li>
<li>age_range | 买家年龄分段：1表示年龄&lt;18,2表示年龄在[18,24]，3表示年龄在[25,29]，4表示年龄在[30,34]，5表示年龄在[35,39]，6表示年龄在[40,49]，7和8表示年龄&gt;=50,0和NULL则表示未知</li>
<li>gender | 性别:0表示女性，1表示男性，2和NULL表示未知</li>
<li>merchant_id | 商家id </li>
<li>label | 是否是回头客，0值表示不是回头客，1值表示回头客，-1值表示该用户已经超出我们所需要考虑的预测范围。NULL值只存在测试集，在测试集中表示需要预测的值。</li>
</ol>
<h2 id="2、数据上传"><a href="#2、数据上传" class="headerlink" title="2、数据上传"></a>2、数据上传</h2><h3 id="1、HDFS创建文件夹-本次实验忽略"><a href="#1、HDFS创建文件夹-本次实验忽略" class="headerlink" title="1、HDFS创建文件夹(本次实验忽略)"></a>1、HDFS创建文件夹(本次实验忽略)</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这个类用于给HDFS创建文件夹</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Ben</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MakeDir</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> BUFFER_SIZE = <span class="number">4096</span>;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		URI uri = <span class="keyword">new</span> URI(<span class="string">"hdfs://192.168.43.73:9000"</span>);</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		conf.set(<span class="string">"fs.hdfs.impl"</span>,</span><br><span class="line">				org.apache.hadoop.hdfs.DistributedFileSystem<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">		FileSystem fs = FileSystem.get(uri, conf);</span><br><span class="line">		<span class="comment">// 打开一个输出流</span></span><br><span class="line">		Path srcPath = <span class="keyword">new</span> Path(<span class="string">"/data"</span>);<span class="comment">//hadoop根目录创建data文件夹</span></span><br><span class="line">		<span class="keyword">boolean</span> isok = fs.mkdirs(srcPath);</span><br><span class="line">		<span class="keyword">if</span> (isok) &#123;</span><br><span class="line">			System.out.println(<span class="string">"Folder creation success!"</span>);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			System.out.println(<span class="string">"Folder creation failed！"</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2、上传数据至Centos"><a href="#2、上传数据至Centos" class="headerlink" title="2、上传数据至Centos"></a>2、上传数据至Centos</h3><p>通过WinSCP，将数据上传至<code>/usr/data</code>中</p>
<h3 id="3、上传数据至HDFS-本次实验忽略"><a href="#3、上传数据至HDFS-本次实验忽略" class="headerlink" title="3、上传数据至HDFS(本次实验忽略)"></a>3、上传数据至HDFS(本次实验忽略)</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.security.UserGroupInformation;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这个类用于给HDFS上传文件</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Ben</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Upload</span> </span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> BUFFER_SIZE = <span class="number">4096</span>;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		URI uri = <span class="keyword">new</span> URI(<span class="string">"hdfs://192.168.43.73:9000"</span>);</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		conf.set(<span class="string">"fs.hdfs.impl"</span>,</span><br><span class="line">				org.apache.hadoop.hdfs.DistributedFileSystem<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">		FileSystem fs = FileSystem.get(uri, conf);</span><br><span class="line"></span><br><span class="line">		Path srcPath = <span class="keyword">new</span> Path(<span class="string">"C:/Users/Ben/Desktop/data/data_format.zip"</span>); <span class="comment">// 原路径</span></span><br><span class="line">		Path dstPath = <span class="keyword">new</span> Path(<span class="string">"/data/"</span>); <span class="comment">// 目标路径</span></span><br><span class="line">		<span class="comment">// 调用文件系统的文件复制函数,前面参数是指是否删除原文件，true为删除，默认为false</span></span><br><span class="line">		fs.copyFromLocalFile(<span class="keyword">false</span>, srcPath, dstPath);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 打印文件路径</span></span><br><span class="line">		System.out.println(<span class="string">"Upload to "</span> + conf.get(<span class="string">"fs.default.name"</span>));</span><br><span class="line">		System.out.println(<span class="string">"------------list files------------"</span> + <span class="string">"\n"</span>);</span><br><span class="line">		FileStatus[] fileStatus = fs.listStatus(dstPath);</span><br><span class="line">		<span class="keyword">for</span> (FileStatus file : fileStatus) &#123;</span><br><span class="line">			System.out.println(file.getPath());</span><br><span class="line">		&#125;</span><br><span class="line">		fs.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3、解压文件"><a href="#3、解压文件" class="headerlink" title="3、解压文件"></a>3、解压文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@bp01 ~]$ cd &#x2F;usr&#x2F;data</span><br><span class="line">[root@bp01 data]$ ls</span><br><span class="line">data_format.zip</span><br><span class="line">&#x2F;&#x2F; 让hadoop拥有操作data的权限</span><br><span class="line">[root@bp01 data]$ sudo chown -R hadoop:hadoop &#x2F;usr&#x2F;data</span><br><span class="line">[root@bp01 data]$ su hadoop</span><br><span class="line">[hadoop@bp01 ~]$ cd &#x2F;usr&#x2F;data</span><br><span class="line">[hadoop@bp01 data]$ unzip data_format.zip</span><br><span class="line">[hadoop@bp01 data]$ ls</span><br><span class="line">data_format.zip  test.csv  train.csv  user_log.csv</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 查看解压后文件前3行数据</span><br><span class="line">[hadoop@bp01 data]$ head -3 user_log.csv</span><br><span class="line">user_id,item_id,cat_id,merchant_id,brand_id,month,day,action,age_range,gender,province</span><br><span class="line">328862,323294,833,2882,2661,08,29,0,0,1,内蒙古</span><br><span class="line">328862,844400,1271,2882,2661,08,29,0,1,1,山西</span><br></pre></td></tr></table></figure>

<h2 id="4、数据预处理"><a href="#4、数据预处理" class="headerlink" title="4、数据预处理"></a>4、数据预处理</h2><h3 id="1、删除标签数据"><a href="#1、删除标签数据" class="headerlink" title="1、删除标签数据"></a>1、删除标签数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 删除第一行标签数据</span><br><span class="line">[hadoop@bp01 data]$ sed -i &#39;1d&#39; user_log.csv</span><br><span class="line">[hadoop@bp01 data]$ head -3 user_log.csv</span><br><span class="line">328862,323294,833,2882,2661,08,29,0,0,1,内蒙古</span><br><span class="line">328862,844400,1271,2882,2661,08,29,0,1,1,山西</span><br><span class="line">328862,575153,1271,2882,2661,08,29,0,2,1,山西</span><br></pre></td></tr></table></figure>
<h3 id="2、查看文件行数"><a href="#2、查看文件行数" class="headerlink" title="2、查看文件行数"></a>2、查看文件行数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 查看文件一个多少行数据</span><br><span class="line">[root@bp01 data]# wc -l user_log.csv</span><br><span class="line">54925330 user_log.csv</span><br></pre></td></tr></table></figure>
<h3 id="3、数据截取"><a href="#3、数据截取" class="headerlink" title="3、数据截取"></a>3、数据截取</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bp01 data]# vi interception.sh</span><br></pre></td></tr></table></figure>
<pre><code>interception.sh文件内容
#!/bin/bash
#下面设置输入文件，把用户执行interception.sh命令时提供的第一个参数作为输入文件名称
infile=$1
#下面设置输出文件，把用户执行interception.sh命令时提供的第二个参数作为输出文件名称
outfile=$2
#注意！！最后的$infile &gt; $outfile必须跟在}’这两个字符的后面
awk -F &quot;,&quot; &apos;BEGIN{
      id=0;
    }
    {
        if($6==11 &amp;&amp; $7==11){
            id=id+1;
            print $1&quot;,&quot;$2&quot;,&quot;$3&quot;,&quot;$4&quot;,&quot;$5&quot;,&quot;$6&quot;,&quot;$7&quot;,&quot;$8&quot;,&quot;$9&quot;,&quot;$10&quot;,&quot;$11
            if(id==100000){
                exit
            }
        }
    }&apos; $infile &gt; $outfile</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@bp01 data]# ls</span><br><span class="line">data_format.zip  interception.sh  test.csv  train.csv  user_log.csv</span><br><span class="line">[root@bp01 data]# chmod +x .&#x2F;interception.sh </span><br><span class="line">[root@bp01 data]# .&#x2F;interception.sh .&#x2F;user_log.csv .&#x2F;mall_user_data.csv</span><br><span class="line">[root@bp01 data]# ls</span><br><span class="line">data_format.zip  interception.sh  mall_user_data.csv  test.csv  train.csv  user_log.csv</span><br><span class="line">[root@bp01 data]# wc -l mall_user_data.csv </span><br><span class="line">100000 mall_user_data.csv</span><br></pre></td></tr></table></figure>

<h1 id="三、Hive实验"><a href="#三、Hive实验" class="headerlink" title="三、Hive实验"></a>三、Hive实验</h1><h2 id="平台启动"><a href="#平台启动" class="headerlink" title="平台启动"></a>平台启动</h2><h3 id="MySQL启动"><a href="#MySQL启动" class="headerlink" title="MySQL启动"></a>MySQL启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bp01 ~]# beeline</span><br><span class="line">beeline&gt; !connect jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306 root !@#123Qaz</span><br></pre></td></tr></table></figure>
<h3 id="Hive启动"><a href="#Hive启动" class="headerlink" title="Hive启动"></a>Hive启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 启动服务 </span><br><span class="line">！！！先进Hadoop用户再启动Hive！！！</span><br><span class="line">[root@bp01 ~]# hive --service metastore &amp; hive --service hiveserver2 &amp;</span><br><span class="line">[root@bp01 ~]# beeline</span><br><span class="line">beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;192.168.43.122:10000 hive !@#123Qaz</span><br></pre></td></tr></table></figure>
<h2 id="数据上传"><a href="#数据上传" class="headerlink" title="数据上传"></a>数据上传</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; create database mall;</span><br><span class="line">No rows affected (0.194 seconds)</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; show databases;</span><br></pre></td></tr></table></figure>
<pre><code>//创建外部表
CREATE EXTERNAL TABLE mall.user_log(
    user_id INT,
    item_id INT,
    cat_id INT,
    merchant_id INT,
    brand_id INT,
    month STRING,
    day STRING,
    action INT,
    age_range INT,
    gender INT,
    province STRING)
    ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;
    STORED AS TEXTFILE LOCATION &apos;/malldata/&apos;;</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; use mall;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; show tables;</span><br></pre></td></tr></table></figure>
<p>此时访问<code>http://192.168.43.73:50070/explorer.html#/</code>存在一个malldata文件夹</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 上传数据</span><br><span class="line">[hadoop@bp01 root]$ hdfs dfs -put &#x2F;usr&#x2F;data&#x2F;mall_user_data.csv &#x2F;malldata</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 查询数据</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select * from user_log limit 10;</span><br><span class="line">&#x2F;&#x2F; 查看表结构和信息</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; desc formatted user_log;</span><br><span class="line">&#x2F;&#x2F; 表结构</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; desc user_log;</span><br><span class="line">+--------------+------------+----------+--+</span><br><span class="line">|   col_name   | data_type  | comment  |</span><br><span class="line">+--------------+------------+----------+--+</span><br><span class="line">| user_id      | int        |          |</span><br><span class="line">| item_id      | int        |          |</span><br><span class="line">| cat_id       | int        |          |</span><br><span class="line">| merchant_id  | int        |          |</span><br><span class="line">| brand_id     | int        |          |</span><br><span class="line">| month        | string     |          |</span><br><span class="line">| day          | string     |          |</span><br><span class="line">| action       | int        |          |</span><br><span class="line">| age_range    | int        |          |</span><br><span class="line">| gender       | int        |          |</span><br><span class="line">| province     | string     |          |</span><br><span class="line">+--------------+------------+----------+--+</span><br></pre></td></tr></table></figure>
<h2 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h2><p>1、用聚合函数count()计算出表内有多少条行数据   </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select count(*) from user_log;</span><br><span class="line">+---------+--+</span><br><span class="line">|   c0    |</span><br><span class="line">+---------+--+</span><br><span class="line">| 100000  |</span><br><span class="line">+---------+--+</span><br></pre></td></tr></table></figure>
<p>2、统计男女用户人数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select gender,count(*) num from user_log group by gender;</span><br><span class="line">+---------+--------+--+</span><br><span class="line">| gender  |  num   |</span><br><span class="line">+---------+--------+--+</span><br><span class="line">| 0       | 33232  |</span><br><span class="line">| 1       | 33215  |</span><br><span class="line">| 2       | 33553  |</span><br><span class="line">+---------+--------+--+</span><br></pre></td></tr></table></figure>
<p>3、统计不同用户行为数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select action,count(*) num from user_log group by action;</span><br><span class="line">+---------+--------+--+</span><br><span class="line">| action  |  num   |</span><br><span class="line">+---------+--------+--+</span><br><span class="line">| 0       | 86357  |</span><br><span class="line">| 1       | 68     |</span><br><span class="line">| 2       | 12476  |</span><br><span class="line">| 3       | 1099   |</span><br><span class="line">+---------+--------+--+</span><br></pre></td></tr></table></figure>
<p>4、男女用户不同年龄段对比</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select gender,age_range,count(*) num from user_log group by gender,age_range;</span><br></pre></td></tr></table></figure>
<p>5、不同省份操作数统计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select province,count(*) num from user_log group by province;</span><br></pre></td></tr></table></figure>
<p>6、不同省份成功成交统计</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select province,count(*) num from user_log where action &#x3D; 2 group by province;</span><br></pre></td></tr></table></figure>
<p>7、访问量前5的商品</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select cat_id,count(*) num from user_log group by cat_id order by num desc limit 5;</span><br></pre></td></tr></table></figure>


<h2 id="查询数据插入临时表"><a href="#查询数据插入临时表" class="headerlink" title="查询数据插入临时表"></a>查询数据插入临时表</h2><p>1、统计男女用户人数(interim_gender)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; </span><br><span class="line">create table interim_gender(gender INT,amount INT) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;</span><br><span class="line">STORED AS TEXTFILE;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; insert overwrite table interim_gender select gender,count(*) num from user_log group by gender;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select * from interim_gender;</span><br></pre></td></tr></table></figure>
<p>2、统计不同买家行为数据(interim_action)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; </span><br><span class="line">create table interim_action(action INT,amount INT) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;</span><br><span class="line">STORED AS TEXTFILE;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; insert overwrite table interim_action select action,count(*) num from user_log group by action;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select * from interim_action;</span><br></pre></td></tr></table></figure>
<p>3、男女用户不同年龄段对比(interim_gender_age)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; </span><br><span class="line">create table interim_gender_age(gender INT,age_range INT,amount INT) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;</span><br><span class="line">STORED AS TEXTFILE;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; insert overwrite table interim_gender_age select gender,age_range,count(*) num from user_log group by gender,age_range;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select * from interim_gender_age;</span><br></pre></td></tr></table></figure>
<p>4、访问量前10的商品(interim_views)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; </span><br><span class="line">create table interim_views(cat_id INT,amount INT) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;</span><br><span class="line">STORED AS TEXTFILE;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; insert overwrite table interim_views select cat_id,count(*) num from user_log group by cat_id order by num desc limit 10;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select * from interim_views;</span><br></pre></td></tr></table></figure>
<p>5、成交量前10的商品(interim_buy)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; </span><br><span class="line">create table interim_buy(cat_id INT,amount INT) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;</span><br><span class="line">STORED AS TEXTFILE;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; insert overwrite table interim_buy select cat_id,count(*) num from user_log where action&#x3D;2 group by cat_id order by num desc limit 10;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select * from interim_buy;</span><br></pre></td></tr></table></figure>
<p>6、不同省份成功成交统计(interim_province_buy)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; </span><br><span class="line">create table interim_province_buy(province STRING,amount INT) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39;</span><br><span class="line">STORED AS TEXTFILE;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; insert overwrite table interim_province_buy select province,count(*) num from user_log where action&#x3D;2 group by province;</span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;192.168.43.73:10000&gt; select * from interim_province_buy;</span><br></pre></td></tr></table></figure>

<h2 id="Sqoop导入MySQL"><a href="#Sqoop导入MySQL" class="headerlink" title="Sqoop导入MySQL"></a>Sqoop导入MySQL</h2><p>1、Mysql</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.73:3306&gt; create database mall;</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.73:3306&gt; use mall;</span><br><span class="line">&#x2F;&#x2F;查看编码</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.73:3306&gt; show variables like &quot;char%&quot;;</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.73:3306&gt; set character_set_server&#x3D;utf8;</span><br></pre></td></tr></table></figure>
<p>2、导入gender数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bp01 sqoop-1.4.7]$ hadoop fs -cat &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mall.db&#x2F;interim_gender&#x2F;000000_0</span><br><span class="line">0	33232</span><br><span class="line">1	33215</span><br><span class="line">2	33553</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; CREATE TABLE &#96;mall&#96;.&#96;gender&#96; (&#96;gender&#96; varchar(10),&#96;amount&#96; varchar(20)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;</span><br><span class="line">[hadoop@bp01 sqoop-1.4.7]$ sqoop export --connect jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&#x2F;mall --username root --password &#39;!@#123Qaz&#39; --table gender --fields-terminated-by &#39;\t&#39; --export-dir &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mall.db&#x2F;interim_gender&#39; -m 1;</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; select * from gender;</span><br></pre></td></tr></table></figure>
<p>3、导入action</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; CREATE TABLE &#96;mall&#96;.&#96;action&#96; (&#96;action&#96; varchar(10),&#96;amount&#96; varchar(20)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;</span><br><span class="line">[hadoop@bp01 sqoop-1.4.7]$ sqoop export --connect jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&#x2F;mall --username root --password &#39;!@#123Qaz&#39; --table action --fields-terminated-by &#39;\t&#39; --export-dir &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mall.db&#x2F;interim_action&#39; -m 1;</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; select * from action;</span><br></pre></td></tr></table></figure>
<p>4、导入gender_age</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; CREATE TABLE &#96;mall&#96;.&#96;gender_age&#96; (&#96;gender&#96; varchar(10),&#96;age_range&#96; varchar(10),&#96;amount&#96; varchar(20)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;</span><br><span class="line">[hadoop@bp01 sqoop-1.4.7]$ sqoop export --connect jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&#x2F;mall --username root --password &#39;!@#123Qaz&#39; --table gender_age --fields-terminated-by &#39;\t&#39; --export-dir &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mall.db&#x2F;interim_gender_age&#39; -m 1;</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; select * from gender_age;</span><br></pre></td></tr></table></figure>
<p>5、导入views</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; CREATE TABLE &#96;mall&#96;.&#96;views&#96; (&#96;cat_id&#96; varchar(20),&#96;amount&#96; varchar(20)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;</span><br><span class="line">[hadoop@bp01 sqoop-1.4.7]$ sqoop export --connect jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&#x2F;mall --username root --password &#39;!@#123Qaz&#39; --table views --fields-terminated-by &#39;\t&#39; --export-dir &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mall.db&#x2F;interim_views&#39; -m 1;</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; select * from views;</span><br></pre></td></tr></table></figure>
<p>6、导入buy</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; CREATE TABLE &#96;mall&#96;.&#96;buy&#96; (&#96;cat_id&#96; varchar(20),&#96;amount&#96; varchar(20)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;</span><br><span class="line">[hadoop@bp01 sqoop-1.4.7]$ sqoop export --connect jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&#x2F;mall --username root --password &#39;!@#123Qaz&#39; --table buy --fields-terminated-by &#39;\t&#39; --export-dir &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mall.db&#x2F;interim_buy&#39; -m 1;</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; select * from buy;</span><br></pre></td></tr></table></figure>
<p>7、导入province_buy</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; CREATE TABLE &#96;mall&#96;.&#96;province_buy&#96; (&#96;province&#96; varchar(10),&#96;amount&#96; varchar(20)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;</span><br><span class="line">[hadoop@bp01 sqoop-1.4.7]$ sqoop export --connect jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&#x2F;mall --username root --password &#39;!@#123Qaz&#39; --table province_buy --fields-terminated-by &#39;\t&#39; --export-dir &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mall.db&#x2F;interim_province_buy&#39; -m 1;</span><br><span class="line">0: jdbc:mysql:&#x2F;&#x2F;192.168.43.122:3306&gt; select * from province_buy;</span><br></pre></td></tr></table></figure>
<hr>
<pre><code>字段解释：
sqoop export  # 表示数据从 hive 复制到 mysql 中
–connect jdbc:mysql://localhost:3306/mall
–username root # mysql登陆用户名
–password *** # 登录密码
–table user_log # mysql 中的表，即将被导入的表名称
–export-dir &apos;/user/hive/warehouse/mall.db/interim_province_buy&apos; #hive中的文件
–fields-terminated-by ‘\n’ #Hive 中被导出的文件字段的分隔符</code></pre><h2 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h2><p>1、错误:Permission denied: user=hive, access=EXECUTE, inode=”/tmp”:hadoop:supergroup:drwx——<br>解决:<code>hadoop fs -chown -R hive:hive  /tmp</code><br>2、错误：The ownership on the staging directory /tmp/hadoop-yarn/staging/hadoop/.staging is not as expected. It is owned by hive. The directory must be owned by the submitter hadoop or by hadoop<br>解决：<code>hadoop fs -chown -R hadoop:hadoop /tmp</code></p>
<h1 id="四、Spark"><a href="#四、Spark" class="headerlink" title="四、Spark"></a>四、Spark</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bp01 ~]# cd &#x2F;usr&#x2F;data</span><br><span class="line">[root@bp01 data]# vi interception_test.sh</span><br></pre></td></tr></table></figure>
<pre><code>#!/bin/bash
#下面设置输入文件，把用户执行interception_test.sh命令时提供的第一个参数作为输入件  名称
infile=$1
#下面设置输出文件，把用户执行interception_test.sh命令时提供的第二个参数作为输出件  名称
outfile=$2
#注意！！最后的$infile &gt; $outfile必须跟在}’这两个字符的后面
awk -F &quot;,&quot; &apos;BEGIN{
      id=0;
    }
    {
        if($1 &amp;&amp; $2 &amp;&amp; $3 &amp;&amp; $4 &amp;&amp; !$5){
            id=id+1;
            print $1&quot;,&quot;$2&quot;,&quot;$3&quot;,&quot;$4&quot;,&quot;1
            if(id==10000){
                exit
            }
        }
    }&apos; $infile &gt; $outfile</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bp01 data]# ls</span><br><span class="line">data_format.zip  interception.sh  interception_test.sh  mall_user_data.csv  test.csv  train.csv  user_log.csv</span><br><span class="line">[root@bp01 data]# chmod +x .&#x2F;interception_test.sh</span><br><span class="line">[root@bp01 data]# .&#x2F;interception_test.sh .&#x2F;test.csv .&#x2F;mall_test.csv、</span><br><span class="line">[root@bp01 data]# wc -l mall_test.csv </span><br><span class="line">10000 mall_test.csv</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bp01 data]# sed -i &#39;1d&#39; train.csv</span><br><span class="line">[root@bp01 data]# vi interception_train.sh</span><br></pre></td></tr></table></figure>
<pre><code>#!/bin/bash
#下面设置输入文件，把用户执行interception_train.sh命令时提供的第一个参数作为输入文件名  称
infile=$1
#下面设置输出文件，把用户执行interception_train.sh命令时提供的第二个参数作为输出文件名  称
outfile=$2
#注意！！最后的$infile &gt; $outfile必须跟在}’这两个字符的后面
awk -F &quot;,&quot; &apos;BEGIN{
         id=0;
    }
    {
        if($1 &amp;&amp; $2 &amp;&amp; $3 &amp;&amp; $4 &amp;&amp; ($5!=-1)){
            id=id+1;
            print $1&quot;,&quot;$2&quot;,&quot;$3&quot;,&quot;$4&quot;,&quot;$5
            if(id==10000){
                exit
            }
        }
    }&apos; $infile &gt; $outfile</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@bp01 data]# chmod +x .&#x2F;interception_train.sh</span><br><span class="line">[root@bp01 data]# .&#x2F;interception_train.sh .&#x2F;train.csv .&#x2F;mall_train.csv</span><br><span class="line">[root@bp01 data]# wc -l mall_train.csv </span><br><span class="line">10000 mall_train.csv</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;spark-shell --jars &#x2F;opt&#x2F;hadoop&#x2F;spark-2.3.1&#x2F;jars&#x2F;mysql-connector-java-5.1.40-bin.jar --driver-class-path &#x2F;opt&#x2F;hadoop&#x2F;spark-2.3.1&#x2F;jars&#x2F;mysql-connector-java-5.1.40-bin.jar</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bp01 spark-2.3.1]$ spark-shell --jars &#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.46-bin.jar --driver-class-path &#x2F;usr&#x2F;local&#x2F;spark&#x2F;jars&#x2F;mysql-connector-java-5.1.46-bin.jar</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.mllib.regression.LabeledPoint</span><br><span class="line">import org.apache.spark.mllib.linalg.&#123;Vectors,Vector&#125;</span><br><span class="line">import org.apache.spark.mllib.classification.&#123;SVMModel, SVMWithSGD&#125;</span><br><span class="line">import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</span><br><span class="line">import java.util.Properties</span><br><span class="line">import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line">val train_data &#x3D; sc.textFile(&quot;&#x2F;usr&#x2F;data&#x2F;mall_train.csv&quot;)</span><br><span class="line">val test_data &#x3D; sc.textFile(&quot;&#x2F;usr&#x2F;data&#x2F;mall_test.csv&quot;)</span><br><span class="line">val train&#x3D; train_data.map&#123;line &#x3D;&gt;</span><br><span class="line">  val parts &#x3D; line.split(&#39;,&#39;)</span><br><span class="line">  LabeledPoint(parts(4).toDouble,Vectors.dense(parts(1).toDouble,parts</span><br><span class="line">(2).toDouble,parts(3).toDouble))</span><br><span class="line">&#125;</span><br><span class="line">val test &#x3D; test_data.map&#123;line &#x3D;&gt;</span><br><span class="line">  val parts &#x3D; line.split(&#39;,&#39;)</span><br><span class="line">  LabeledPoint(parts(4).toDouble,Vectors.dense(parts(1).toDouble,parts(2).toDouble,parts(3).toDouble))</span><br><span class="line">&#125;</span><br><span class="line">val numIterations &#x3D; 1000</span><br><span class="line">val model &#x3D; SVMWithSGD.train(train, numIterations)</span><br><span class="line">model.clearThreshold()</span><br><span class="line">val scoreAndLabels &#x3D; test.map&#123;point &#x3D;&gt;</span><br><span class="line">  val score &#x3D; model.predict(point.features)</span><br><span class="line">  score+&quot; &quot;+point.label</span><br><span class="line">&#125;</span><br><span class="line">scoreAndLabels.foreach(println)</span><br><span class="line"></span><br><span class="line">model.setThreshold(0.0)</span><br><span class="line">scoreAndLabels.foreach(println)</span><br><span class="line"></span><br><span class="line">model.clearThreshold()</span><br><span class="line">val scoreAndLabels &#x3D; test.map&#123;point &#x3D;&gt;</span><br><span class="line">  val score &#x3D; model.predict(point.features)</span><br><span class="line">  score+&quot; &quot;+point.label</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F;设置回头客数据</span><br><span class="line">val rebuyRDD &#x3D; scoreAndLabels.map(_.split(&quot; &quot;))</span><br><span class="line">&#x2F;下面要设置模式信息</span><br><span class="line">val schema &#x3D; StructType(List(StructField(&quot;score&quot;, StringType, true),StructField(&quot;label&quot;, StringType, true)))</span><br><span class="line">&#x2F;&#x2F;下面创建Row对象，每个Row对象都是rowRDD中的一行</span><br><span class="line">val rowRDD &#x3D; rebuyRDD.map(p &#x3D;&gt; Row(p(0).trim, p(1).trim))</span><br><span class="line">&#x2F;&#x2F;建立起Row对象和模式之间的对应关系，也就是把数据和模式对应起来</span><br><span class="line">val rebuyDF &#x3D; spark.createDataFrame(rowRDD, schema)</span><br><span class="line">&#x2F;&#x2F;下面创建一个prop变量用来保存JDBC连接参数</span><br><span class="line">val prop &#x3D; new Properties()</span><br><span class="line">prop.put(&quot;user&quot;, &quot;root&quot;) &#x2F;&#x2F;表示用户名是root</span><br><span class="line">prop.put(&quot;password&quot;, &quot;!@#123Qaz&quot;) &#x2F;&#x2F;表示密码是hadoop</span><br><span class="line">prop.put(&quot;driver&quot;,&quot;com.mysql.jdbc.Driver&quot;) &#x2F;&#x2F;表示驱动程序是com.mysql.jdbc.Driver</span><br><span class="line">&#x2F;&#x2F;下面就可以连接数据库，采用append模式，表示追加记录到数据库mall的rebuy表中</span><br><span class="line">rebuyDF.write.mode(&quot;append&quot;).jdbc(&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mall&quot;, &quot;mall.rebuy&quot;, prop)</span><br></pre></td></tr></table></figure>

<hr>
<p>参考：<a href="http://dblab.xmu.edu.cn/blog/1363-2/" target="_blank" rel="noopener">http://dblab.xmu.edu.cn/blog/1363-2/</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Ben</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://smallbenxiong.github.io/2019/12/26/20191226-%E7%94%B5%E5%95%86%E5%B9%B3%E5%8F%B0%E5%8F%8C11%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">https://smallbenxiong.github.io/2019/12/26/20191226-%E7%94%B5%E5%95%86%E5%B9%B3%E5%8F%B0%E5%8F%8C11%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://smallbenxiong.github.io">Ben Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop    </a><a class="post-meta__tags" href="/tags/Hive/">Hive    </a><a class="post-meta__tags" href="/tags/Spark/">Spark    </a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析    </a></div><div class="post_share"><div class="social-share" data-image="https://img.alicdn.com/tfs/TB1MaLKRXXXXXaWXFXXXXXXXXXX-480-260.png" data-sites="linkedin,google,facebook,twitter,weibo,wechat,qq,qzone"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">WeChat</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">Alipay</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><img class="prev_cover lazyload" data-src="https://www.gstatic.cn/devrel-devsite/prod/v18acaed6c9e1965881c7cfa6f3f9f69e7c37603f6d253114168821ce9e70c645/tensorflow/images/lockup.svg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>TensorFlow | TensorFlow实现单变量线性回归</span></div></a></div><div class="next-post pull_right"><a href="/2019/12/20/20191220-Sqoop%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/"><img class="next_cover lazyload" data-src="https://hadoop.apache.org/hadoop-logo.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>Sqoop | Sqoop报错解决</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/12/20/20191220-Sqoop报错解决/" title="Sqoop | Sqoop报错解决"><img class="relatedPosts_cover lazyload"data-src="https://hadoop.apache.org/hadoop-logo.jpg"><div class="relatedPosts_title">Sqoop | Sqoop报错解决</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '136904656b3e77920595',
  clientSecret: 'ecdc12b452f40a56859f3bc90eced4fdd0ee9c15',
  repo: 'blog-comments',
  owner: 'smallbenxiong',
  admin: 'smallbenxiong',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN'
})
gitalk.render('gitalk-container')</script></div></div></div><footer id="footer" style="background-image: url(https://img.alicdn.com/tfs/TB1MaLKRXXXXXaWXFXXXXXXXXXX-480-260.png)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By Ben</div><div class="footer_custom_text">Hi, welcome to my <a href="https://smallbenxiong.github.io/">blog</a>!</div><div class="icp"><a href="https://smallbenxiong.github.io/"><img class="icp-icon" src="/img/icp.png"><span>晋ICP备19009621号-1</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script async src="/js/search/algolia.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.15/dist/snackbar.min.js"></script><script id="ribbon_piao" mobile="true" src="/js/third-party/piao.js"></script><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>