<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ben Blog</title>
  
  <subtitle>一个无聊的博客。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://smallbenxiong.github.io/"/>
  <updated>2020-01-06T14:25:11.168Z</updated>
  <id>https://smallbenxiong.github.io/</id>
  
  <author>
    <name>Ben</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorFlow | TensorFlow实现单变量线性回归</title>
    <link href="https://smallbenxiong.github.io/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>https://smallbenxiong.github.io/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</id>
    <published>2020-01-05T16:00:00.000Z</published>
    <updated>2020-01-06T14:25:11.168Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="TensorFlow实现单变量线性回归"><a href="#TensorFlow实现单变量线性回归" class="headerlink" title="TensorFlow实现单变量线性回归"></a>TensorFlow实现单变量线性回归</h1><p><strong>学习的函数为线性函数 y=2x+1</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Jupyter中显示图像需使用matplotlib的inline模式</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="生成人工数据集"><a href="#生成人工数据集" class="headerlink" title="生成人工数据集"></a>生成人工数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置随机种子</span></span><br><span class="line">np.random.seed(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接采用np生成等差数列的方法，生成100个点，每个点的取值在-1~1之间</span></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y = 2x + 1 + 噪声,其中噪声的维度与x_data一致。随机振幅，幅度设为0.4</span></span><br><span class="line">y_data = <span class="number">2</span> * x_data + <span class="number">1.0</span> + np.random.randn(*x_data.shape) * <span class="number">0.4</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy.randoom.randn(d0,d1,…,dn)是从标准正态分布中返回一个或多个样本值</span></span><br><span class="line"><span class="comment"># 标准正态分布又称u分布，是以0为均值，1为标准差的正态分布，记为N(0, 1)</span></span><br><span class="line"></span><br><span class="line">np.random.randn(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>array([ 0.79242262,  0.17076445, -1.75374086,  0.63029648,  0.49832921,        1.01813761, -0.84646862,  2.52080763, -1.23238611,  0.72695326])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x_data.shape值为一个元组</span></span><br><span class="line">x_data.shape</span><br></pre></td></tr></table></figure><pre><code>(100,)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实参前面加上*或者**时，意味着拆包。单个*表示将元组拆成一个个单独的实参</span></span><br><span class="line"><span class="comment"># np.random.randn(*x_data.shape)和np.random.randn(100)一样</span></span><br><span class="line"></span><br><span class="line">np.random.randn(*x_data.shape)</span><br></pre></td></tr></table></figure><pre><code>array([ 0.04595522, -0.48713265,  0.81613236, -0.28143012, -2.33562182,       -1.16727845,  0.45765807,  2.23796561, -1.4812592 , -0.01694532,        1.45073354,  0.60687032, -0.37562084, -1.42192455, -1.7811513 ,       -0.74790579, -0.36840953, -2.24911813, -1.69367504,  0.30364847,       -0.40899234, -0.75483059, -0.40751917, -0.81262476,  0.92751621,        1.63995407,  2.07361553,  0.70979786,  0.74715259,  1.46309548,        1.73844881,  1.46520488,  1.21228341, -0.6346525 , -1.5996985 ,        0.87715281, -0.09383245, -0.05567103, -0.88942073, -1.30095145,        1.40216662,  0.46510099, -1.06503262,  0.39042061,  0.30560017,        0.52184949,  2.23327081, -0.0347021 , -1.27962318,  0.03654264,       -0.64635659,  0.54856784,  0.21054246,  0.34650175, -0.56705117,        0.41367881, -0.51025606,  0.51725935, -0.30100513, -1.11840643,        0.49852362, -0.70609387,  1.4438811 ,  0.44295626,  0.46770521,        0.10134479, -0.05935198, -2.38669774,  1.22217056, -0.81391201,        0.95626186, -0.63851056, -0.14312642, -0.22418983, -1.03849524,       -0.17170905,  0.47634618, -0.41417827, -1.26408334, -0.57321556,        0.24981732,  1.14720208,  0.83594396,  0.28740365, -0.9955963 ,        0.90688947,  0.02421074, -0.23998173,  0.91011056,  0.61784475,        0.49961804, -1.15154425, -0.6105164 , -1.70388541,  0.19443738,        0.02824125,  0.93256051,  0.21204332, -0.36794457,  2.1114884 ])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.randn(<span class="number">100</span>)</span><br></pre></td></tr></table></figure><pre><code>array([-1.02957349, -1.33628031, -0.61056736,  0.52469426, -0.34930813,       -0.44073846, -1.1212876 ,  1.47284473, -0.62337224, -1.08070195,       -0.12253009, -0.8077431 , -0.23255622,  1.33515034, -0.44645673,       -0.04978868, -0.36854478, -0.19173957,  0.81967992,  0.53163372,       -0.34161504, -0.93090048, -0.13421699,  0.83259361, -0.01735327,       -0.12765822, -1.80791662,  0.99396898, -1.49112886, -1.28210748,       -0.37570741,  0.03464388,  0.04507816, -0.76374689, -0.31313851,       -0.60698954, -1.80955123, -0.25551774, -0.69379935,  0.41919776,       -0.14520019,  0.9638013 ,  0.69622199,  0.89940546,  1.20837807,        0.6932537 , -0.16636061,  1.35311311, -0.92862651, -0.03547249,        0.85964595, -0.28749661,  0.71494995, -0.8034526 , -0.54048196,        0.54617743,  0.71188926,  1.19715449, -0.07006703,  0.29822712,        0.62619261,  0.46743206, -1.30262143, -0.57008965,  1.44295001,       -1.24399513,  0.62888033, -0.42559213,  1.00320956, -0.77817761,        0.04894463, -2.02640189, -0.04193635,  1.07454278, -1.5008594 ,        1.18574443, -0.71508124, -0.05123853, -2.77458336,  1.07862813,       -0.87568592, -0.53810932, -1.2782157 , -0.99276945,  1.14342789,       -0.5090726 ,  0.89500094, -0.17620337,  0.34608347, -0.50631013,        0.42716402,  2.58856959,  0.65289301,  0.50583979, -0.47595083,        1.01090874,  1.35920097, -1.70208997, -1.38033223,  2.10177668])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y = 2x+1+噪声</span></span><br><span class="line">y_data = <span class="number">2</span> * x_data + <span class="number">1.0</span> + np.random.randn(<span class="number">100</span>) * <span class="number">0.4</span></span><br></pre></td></tr></table></figure><h3 id="matplotlib画出生成结果"><a href="#matplotlib画出生成结果" class="headerlink" title="matplotlib画出生成结果"></a>matplotlib画出生成结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画出随机生成数据散点图</span></span><br><span class="line">plt.scatter(x_data, y_data)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.collections.PathCollection at 0x22c32a84c18&gt;</code></pre><p><img alt="png" data-src="/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_11_1.png" class="lazyload"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画出想要学习到的线性函数y = 2x + 1</span></span><br><span class="line">plt.scatter(x_data, y_data)</span><br><span class="line">plt.plot(x_data, <span class="number">1.0</span> + <span class="number">2</span> * x_data, color = <span class="string">'red'</span>, linewidth=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x22c32b14f28&gt;]</code></pre><p><img alt="png" data-src="/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_12_1.png" class="lazyload"></p><h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><h3 id="定义x和y的占位符"><a href="#定义x和y的占位符" class="headerlink" title="定义x和y的占位符"></a>定义x和y的占位符</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练数据的占位符，x是特征值，y是标签纸</span></span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"x"</span>)</span><br><span class="line">y = tf.placeholder(<span class="string">"float"</span>, name = <span class="string">"y"</span>)</span><br></pre></td></tr></table></figure><h3 id="构建回归模型"><a href="#构建回归模型" class="headerlink" title="构建回归模型"></a>构建回归模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型函数</span></span><br><span class="line"><span class="comment"># 通过训练模型求出最合适的w，b，使总的损失最小</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.multiply(x, w) + b <span class="comment"># w*x+b</span></span><br></pre></td></tr></table></figure><h3 id="创建变量"><a href="#创建变量" class="headerlink" title="创建变量"></a>创建变量</h3><ul><li>变量声明函数使tf.Variable</li><li>变量作用是保存和更新模型参数   </li><li>变量的初始化可以是随机数、常数，或者其他变量的初始值计算得到</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建变量</span></span><br><span class="line"><span class="comment"># 构建线性函数的斜率，变量w</span></span><br><span class="line">w = tf.Variable(<span class="number">1.0</span>, name = <span class="string">"w0"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建线性函数的截距，变量b</span></span><br><span class="line">b = tf.Variable(<span class="number">0.0</span>, name = <span class="string">"b0"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pred是预测值，前向计算</span></span><br><span class="line">pred = model(x, w, b) <span class="comment">#即wx+b的计算值</span></span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><h3 id="设置训练参数"><a href="#设置训练参数" class="headerlink" title="设置训练参数"></a>设置训练参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 迭代次数（训练轮数）</span></span><br><span class="line">train_epochs = <span class="number">10</span></span><br></pre></td></tr></table></figure><p>关于学习率（learning_rate）的设置</p><ul><li>学习率作用：控制参数更新的幅度</li><li>学习率设置过大：可能导致参数在极值附件来回摇摆，无法保证收敛</li><li>学习率设置国小：虽然能保证收敛，但是优化速度大大降低，需要迭代次数更多次数才能达到比较理想的优化效果</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 学习率</span></span><br><span class="line">learning_rate = <span class="number">0.05</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 控制显示loss值的粒度</span></span><br><span class="line">display_step = <span class="number">10</span></span><br></pre></td></tr></table></figure><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><ul><li>损失函数用于描述预测值和真实值之间的误差，从而指导模型收敛方向</li><li>常见损失函数：均方差（Mean Square Errir,MSE）和交叉熵（cross-entropy）</li></ul><p>$ L_2 损失函数 $</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用均方差作为损失函数</span></span><br><span class="line"></span><br><span class="line">loss_function = tf.reduce_mean(tf.square(y-pred)) <span class="comment"># reduce_mean平均值，square平方</span></span><br></pre></td></tr></table></figure><h3 id="定义优化器"><a href="#定义优化器" class="headerlink" title="定义优化器"></a>定义优化器</h3><ul><li>定义优化器Optimizer，初始化一个GradientDescentOptimizer</li><li>设置学习率和优化目标：最小化损失</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降优化器</span></span><br><span class="line"><span class="comment"># GradientDescentOptimizer(learning_rate)学习率learning_rate来指导优化</span></span><br><span class="line"><span class="comment"># minimize(loss_function)把损失函数loss_function最小化</span></span><br><span class="line"></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)</span><br></pre></td></tr></table></figure><h3 id="创建会话"><a href="#创建会话" class="headerlink" title="创建会话"></a>创建会话</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明会话</span></span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 变量初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># init是一个节点，需要run</span></span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure><h3 id="迭代训练"><a href="#迭代训练" class="headerlink" title="迭代训练"></a>迭代训练</h3><p>模型训练阶段设置迭代轮次，每次通过将样本逐个输入模型，进行梯度下降优化操作，每轮迭代后绘制出模型曲线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示损失值的训练</span></span><br><span class="line"><span class="comment"># 开始训练，轮次为epoch，采用SGD随机梯度下降优化方法</span></span><br><span class="line">step = <span class="number">0</span>   <span class="comment"># 记录训练步数</span></span><br><span class="line">loss_list = []  <span class="comment"># 用于保存loss值的列表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    <span class="keyword">for</span> xs, ys <span class="keyword">in</span> zip(x_data, y_data):</span><br><span class="line">        <span class="comment"># 模型优化，运行两个节点优化器和损失函数</span></span><br><span class="line">        <span class="comment"># 给占位符x和y填充真实值xs和ys</span></span><br><span class="line">        _, loss = sess.run([optimizer, loss_function], feed_dict=&#123;x:xs, y:ys&#125;)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 显示损失值loss</span></span><br><span class="line">        <span class="comment"># display_step：控制报告的粒度</span></span><br><span class="line">        <span class="comment"># 例如，如果display_step设为2，则将每训练2个样本输出一次损失值</span></span><br><span class="line">        <span class="comment"># 与超参数不同，修改display_step 不会更改模型所学习的规律</span></span><br><span class="line">        loss_list.append(loss)</span><br><span class="line">        step = step+<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> step % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Train Epoch:"</span>,<span class="string">'%02d'</span> % (epoch+<span class="number">1</span>), <span class="string">"Step:%03d"</span> % (step),<span class="string">"loss="</span>,<span class="string">"&#123;:.9f&#125;"</span>.format(loss))</span><br><span class="line">        </span><br><span class="line">    b0temp = b.eval(session=sess)</span><br><span class="line">    w0temp = w.eval(session=sess)</span><br><span class="line">    plt.plot (x_data, w0temp * x_data + b0temp) <span class="comment">#画图</span></span><br></pre></td></tr></table></figure><pre><code>Train Epoch: 01 Step:010 loss= 0.053888980Train Epoch: 01 Step:020 loss= 0.000218245Train Epoch: 01 Step:030 loss= 0.019443041Train Epoch: 01 Step:040 loss= 0.589532554Train Epoch: 01 Step:050 loss= 0.000989183Train Epoch: 01 Step:060 loss= 0.142488658Train Epoch: 01 Step:070 loss= 0.046271212Train Epoch: 01 Step:080 loss= 0.008660123Train Epoch: 01 Step:090 loss= 0.241159379Train Epoch: 01 Step:100 loss= 0.000514947Train Epoch: 02 Step:110 loss= 0.317517459Train Epoch: 02 Step:120 loss= 0.032397330Train Epoch: 02 Step:130 loss= 0.093368128Train Epoch: 02 Step:140 loss= 0.332103789Train Epoch: 02 Step:150 loss= 0.060521714Train Epoch: 02 Step:160 loss= 0.024084859Train Epoch: 02 Step:170 loss= 0.178793266Train Epoch: 02 Step:180 loss= 0.006461896Train Epoch: 02 Step:190 loss= 0.129687995Train Epoch: 02 Step:200 loss= 0.013333416Train Epoch: 03 Step:210 loss= 0.129900724Train Epoch: 03 Step:220 loss= 0.023582600Train Epoch: 03 Step:230 loss= 0.096030191Train Epoch: 03 Step:240 loss= 0.317024857Train Epoch: 03 Step:250 loss= 0.069221057Train Epoch: 03 Step:260 loss= 0.018716505Train Epoch: 03 Step:270 loss= 0.193809599Train Epoch: 03 Step:280 loss= 0.009021518Train Epoch: 03 Step:290 loss= 0.121858403Train Epoch: 03 Step:300 loss= 0.015201909Train Epoch: 04 Step:310 loss= 0.117845014Train Epoch: 04 Step:320 loss= 0.022902815Train Epoch: 04 Step:330 loss= 0.096256405Train Epoch: 04 Step:340 loss= 0.315768689Train Epoch: 04 Step:350 loss= 0.069981642Train Epoch: 04 Step:360 loss= 0.018294554Train Epoch: 04 Step:370 loss= 0.195104137Train Epoch: 04 Step:380 loss= 0.009256961Train Epoch: 04 Step:390 loss= 0.121209100Train Epoch: 04 Step:400 loss= 0.015365199Train Epoch: 05 Step:410 loss= 0.116854727Train Epoch: 05 Step:420 loss= 0.022845931Train Epoch: 05 Step:430 loss= 0.096275523Train Epoch: 05 Step:440 loss= 0.315662980Train Epoch: 05 Step:450 loss= 0.070045985Train Epoch: 05 Step:460 loss= 0.018259227Train Epoch: 05 Step:470 loss= 0.195213363Train Epoch: 05 Step:480 loss= 0.009276883Train Epoch: 05 Step:490 loss= 0.121154651Train Epoch: 05 Step:500 loss= 0.015378974Train Epoch: 06 Step:510 loss= 0.116771445Train Epoch: 06 Step:520 loss= 0.022841139Train Epoch: 06 Step:530 loss= 0.096277155Train Epoch: 06 Step:540 loss= 0.315654010Train Epoch: 06 Step:550 loss= 0.070051350Train Epoch: 06 Step:560 loss= 0.018256264Train Epoch: 06 Step:570 loss= 0.195222735Train Epoch: 06 Step:580 loss= 0.009278628Train Epoch: 06 Step:590 loss= 0.121149838Train Epoch: 06 Step:600 loss= 0.015380217Train Epoch: 07 Step:610 loss= 0.116764441Train Epoch: 07 Step:620 loss= 0.022840742Train Epoch: 07 Step:630 loss= 0.096277267Train Epoch: 07 Step:640 loss= 0.315653324Train Epoch: 07 Step:650 loss= 0.070051797Train Epoch: 07 Step:660 loss= 0.018256038Train Epoch: 07 Step:670 loss= 0.195223376Train Epoch: 07 Step:680 loss= 0.009278720Train Epoch: 07 Step:690 loss= 0.121149674Train Epoch: 07 Step:700 loss= 0.015380275Train Epoch: 08 Step:710 loss= 0.116763875Train Epoch: 08 Step:720 loss= 0.022840688Train Epoch: 08 Step:730 loss= 0.096277334Train Epoch: 08 Step:740 loss= 0.315653145Train Epoch: 08 Step:750 loss= 0.070051856Train Epoch: 08 Step:760 loss= 0.018255942Train Epoch: 08 Step:770 loss= 0.195223376Train Epoch: 08 Step:780 loss= 0.009278720Train Epoch: 08 Step:790 loss= 0.121149503Train Epoch: 08 Step:800 loss= 0.015380275Train Epoch: 09 Step:810 loss= 0.116763793Train Epoch: 09 Step:820 loss= 0.022840688Train Epoch: 09 Step:830 loss= 0.096277334Train Epoch: 09 Step:840 loss= 0.315653145Train Epoch: 09 Step:850 loss= 0.070051923Train Epoch: 09 Step:860 loss= 0.018255910Train Epoch: 09 Step:870 loss= 0.195223585Train Epoch: 09 Step:880 loss= 0.009278766Train Epoch: 09 Step:890 loss= 0.121149339Train Epoch: 09 Step:900 loss= 0.015380275Train Epoch: 10 Step:910 loss= 0.116763711Train Epoch: 10 Step:920 loss= 0.022840671Train Epoch: 10 Step:930 loss= 0.096277304Train Epoch: 10 Step:940 loss= 0.315653145Train Epoch: 10 Step:950 loss= 0.070051856Train Epoch: 10 Step:960 loss= 0.018255973Train Epoch: 10 Step:970 loss= 0.195223376Train Epoch: 10 Step:980 loss= 0.009278720Train Epoch: 10 Step:990 loss= 0.121149503Train Epoch: 10 Step:1000 loss= 0.015380275</code></pre><p><img alt="png" data-src="/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_40_1.png" class="lazyload"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(loss_list)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x22c32cef9e8&gt;]</code></pre><p><img alt="png" data-src="/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_41_1.png" class="lazyload"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(loss_list,<span class="string">'r+'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x22c32d45ba8&gt;]</code></pre><p><img alt="png" data-src="/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_42_1.png" class="lazyload"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[x <span class="keyword">for</span> x <span class="keyword">in</span> loss_list <span class="keyword">if</span> x&gt;<span class="number">1</span>]</span><br></pre></td></tr></table></figure><pre><code>[1.4533501, 1.3507473, 1.7046989, 2.2887022, 1.7251762, 1.9852284, 1.1750387, 1.7792182, 1.1360258, 1.7623546, 1.132765, 1.7609351, 1.1324903, 1.7608157, 1.1324672, 1.7608054, 1.1324654, 1.7608048, 1.1324649, 1.7608048, 1.1324646, 1.7608044, 1.1324649]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line">print(<span class="string">"w:"</span>, sess.run(w)) <span class="comment"># w的值应该在2附件</span></span><br><span class="line">print(<span class="string">"b:"</span>, sess.run(b)) <span class="comment"># b的值应该在1附近</span></span><br></pre></td></tr></table></figure><pre><code>w: 1.9070293b: 1.0205086</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">plt.scatter(x_data, y_data, label=<span class="string">"Original data"</span>)</span><br><span class="line">plt.plot(x_data,x_data * sess.run(w)+sess.run(b),label = <span class="string">"Fitted line"</span>, color = <span class="string">'r'</span>,linewidth = <span class="number">3</span>)</span><br><span class="line">plt.legend(loc = <span class="number">2</span>) <span class="comment"># 通过参数loc指定图例位置,左上角标签显示</span></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x22c32da9c50&gt;</code></pre><p><img alt="png" data-src="/2020/01/06/20200106-TensorFlow%E5%AE%9E%E7%8E%B0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/output_45_1.png" class="lazyload"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> xs,ys <span class="keyword">in</span> zip(x_data, y_data):</span><br><span class="line">    print(xs, ys)</span><br></pre></td></tr></table></figure><pre><code>-1.0 -0.8296403329862183-0.9797979797979798 -0.907915867983997-0.9595959595959596 -0.6940069139116888-0.9393939393939394 -0.44008198904613316-0.9191919191919192 -0.518056298082633-0.898989898989899 -0.8872131046936622-0.8787878787878788 0.06789250806751634-0.8585858585858586 -0.7121223166059172-0.8383838383838383 -1.0266771956105798-0.8181818181818181 -0.8591953886106469-0.797979797979798 -0.6488803761421159-0.7777777777777778 -0.7072455496370039-0.7575757575757576 -0.5902689976300266-0.7373737373737373 -0.3485610146594407-0.7171717171717171 -1.7479096133940975-0.696969696969697 -0.7054166950286986-0.6767676767676767 0.02660252968277821-0.6565656565656566 -0.0879307925836007-0.6363636363636364 -0.5449888988689496-0.6161616161616161 -0.4821245885002341-0.5959595959595959 0.26427918331828665-0.5757575757575757 -0.2506067288023764-0.5555555555555556 -0.3231932210702846-0.5353535353535352 0.6715786910122715-0.5151515151515151 -0.17825188097185915-0.4949494949494949 0.2837997365730506-0.4747474747474747 0.03223693168166275-0.4545454545454545 0.11122375083990527-0.43434343434343425 -0.24757709110216308-0.41414141414141414 -0.15907779562657415-0.3939393939393939 0.7153506547917942-0.3737373737373737 -0.20303085077192035-0.3535353535353535 0.4038733399023537-0.33333333333333326 -0.14444505243328032-0.31313131313131304 0.2775626397745727-0.2929292929292928 0.39925810781510473-0.2727272727272727 -0.19732590501344383-0.2525252525252525 0.9488948450886014-0.23232323232323226 0.46183376274541277-0.21212121212121204 1.0616526671192374-0.19191919191919182 0.24245017816545178-0.1717171717171716 1.0213106217668546-0.1515151515151515 1.6306657350096654-0.13131313131313127 0.8239865196438662-0.11111111111111105 0.5189503868361454-0.09090909090909083 1.0096642401086826-0.07070707070707061 0.9149167196744973-0.050505050505050386 0.8149904585811949-0.030303030303030276 0.8911912817877188-0.010101010101010055 0.74826766678925940.010101010101010166 1.18974905751868580.030303030303030498 0.90567351526606330.05050505050505061 0.75826284168208970.07070707070707072 1.46753969864337640.09090909090909105 1.11549177457064670.11111111111111116 2.28036360037980760.1313131313131315 1.16283831200039220.1515151515151516 1.01609514505545740.1717171717171717 1.12363541048898670.19191919191919204 1.53227316407232880.21212121212121215 1.1453041019262370.2323232323232325 0.95932599938001550.2525252525252526 2.1607354389680980.27272727272727293 1.0493966910905730.29292929292929304 2.19131689305766160.31313131313131315 1.6826852528906240.3333333333333335 1.24182748311133850.3535353535353536 2.3736228676983310.3737373737373739 1.6660933058664980.39393939393939403 1.38485977664746620.41414141414141437 1.85444665199489460.4343434343434345 2.3832651640036880.4545454545454546 2.18258823597854820.4747474747474749 2.27999286786918940.49494949494949503 2.69163351959694850.5151515151515154 2.116128624114660.5353535353535355 2.22103298487163150.5555555555555556 2.02071744491074150.5757575757575759 1.56963760307356330.595959595959596 2.134105328006630.6161616161616164 2.87911475452715050.6363636363636365 1.58030582148225430.6565656565656568 2.85070976699570530.6767676767676769 2.45815118309388270.696969696969697 2.02973165236894550.7171717171717173 2.46053123064597120.7373737373737375 3.1852763264068410.7575757575757578 2.28589823980159720.7777777777777779 2.87179300071438660.7979797979797982 3.0485651854539460.8181818181818183 2.27752603736069140.8383838383838385 2.92985242154697460.8585858585858588 2.93123377822040960.8787878787878789 2.56791479345059330.8989898989898992 3.07197416373106340.9191919191919193 2.69390616428374850.9393939393939394 2.64781590499802370.9595959595959598 2.4253001195389720.9797979797979799 3.11378791689627481.0 2.82832400301817</code></pre><h2 id="进行预测"><a href="#进行预测" class="headerlink" title="进行预测"></a>进行预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x_test = <span class="number">3.21</span></span><br><span class="line"></span><br><span class="line">predict = sess.run(pred, feed_dict = &#123;x:x_test&#125;)</span><br><span class="line">print(<span class="string">"预测值：%f"</span> %predict)</span><br><span class="line"></span><br><span class="line">target = <span class="number">2</span> * x_test +<span class="number">1.0</span></span><br><span class="line">print(<span class="string">"目标值：%f"</span> %target)</span><br></pre></td></tr></table></figure><pre><code>预测值：7.142073目标值：7.420000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 等价于上面的predict</span></span><br><span class="line">x_test = <span class="number">3.21</span></span><br><span class="line">predict = sess.run(w) * x_test + sess.run(b)</span><br><span class="line">print(<span class="string">"预测值：%f"</span> %predict)</span><br></pre></td></tr></table></figure><pre><code>预测值：7.142073</code></pre>]]></content>
    
    <summary type="html">
    
      TensorFlow实现单变量线性回归
    
    </summary>
    
    
      <category term="TensorFlow" scheme="https://smallbenxiong.github.io/categories/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="https://smallbenxiong.github.io/tags/TensorFlow/"/>
    
      <category term="深度学习" scheme="https://smallbenxiong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="https://smallbenxiong.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop | Sqoop报错解决</title>
    <link href="https://smallbenxiong.github.io/2019/12/20/20191220-Sqoop%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/"/>
    <id>https://smallbenxiong.github.io/2019/12/20/20191220-Sqoop%E6%8A%A5%E9%94%99%E8%A7%A3%E5%86%B3/</id>
    <published>2019-12-19T16:00:00.000Z</published>
    <updated>2019-12-20T01:54:51.873Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="Sqoop-Sqoop报错解决"><a href="#Sqoop-Sqoop报错解决" class="headerlink" title="Sqoop | Sqoop报错解决"></a>Sqoop | Sqoop报错解决</h1><h2 id="Job-failed-as-tasks-failed"><a href="#Job-failed-as-tasks-failed" class="headerlink" title="Job failed as tasks failed."></a>Job failed as tasks failed.</h2><pre><code>19/12/20 06:43:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1576794672412_000119/12/20 06:43:58 INFO impl.YarnClientImpl: Submitted application application_1576794672412_000119/12/20 06:43:58 INFO mapreduce.Job: The url to track the job: http://bp01:8088/proxy/application_1576794672412_0001/19/12/20 06:43:58 INFO mapreduce.Job: Running job: job_1576794672412_000119/12/20 06:44:17 INFO mapreduce.Job: Job job_1576794672412_0001 running in uber mode : false19/12/20 06:44:17 INFO mapreduce.Job:  map 0% reduce 0%19/12/20 06:44:25 INFO mapreduce.Job:  map 100% reduce 0%19/12/20 06:44:27 INFO mapreduce.Job: Job job_1576794672412_0001 failed with state FAILED due to: Task failed   task_1576794672412_0001_m_000000Job failed as tasks failed. failedMaps:1 failedReduces:0</code></pre><p><strong>解决：<br>查看日志:/opt/hadoop/hadoop-2.7.6/logs/userlogs/application_1576794672412_0006/container_1576794672412_0006_01_000002</strong>   </p><pre><code>2019-12-20 07:12:27,425 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException:   com.mysql.jdbc.MysqlDataTruncation: Data truncation: Data too long for column  &apos;gender&apos; at row 1Caused by: com.mysql.jdbc.MysqlDataTruncation: Data truncation: Data too long   for column &apos;gender&apos; at row 1</code></pre><p><strong>！！ Data too long for column  ‘gender’ at row 1</strong></p><p>参考：<a href="https://blog.csdn.net/sunjinjuan/article/details/88854091" target="_blank" rel="noopener">sqoop从hive导出到mysql报错：failed with state FAILED due to: Task failed</a></p><hr><h2 id="Hive导入MySQL的数据全部出现在同一列"><a href="#Hive导入MySQL的数据全部出现在同一列" class="headerlink" title="Hive导入MySQL的数据全部出现在同一列"></a>Hive导入MySQL的数据全部出现在同一列</h2><pre><code>Hive数据+------------------------+------------------------+--+| interim_gender.gender  | interim_gender.amount  |+------------------------+------------------------+--+| 0                      | 33232                  || 1                      | 33215                  || 2                      | 33553                  |+------------------------+------------------------+--+</code></pre><p>导入：sqoop export –connect jdbc:mysql://192.168.43.122:3306/mall –username root –password *** –table gender –input-fields-terminated-by ‘\001’ –export-dir ‘/user/hive/warehouse/mall.db/interim_gender’ -m 1;</p><pre><code>MySQL数据+----------+---------+--+|  gender  | amount  |+----------+---------+--+| 1    33215  | NULL    || 2    33553  | NULL    || 0    33232  | NULL    |+----------+---------+--+</code></pre><p><strong>解决：<br>查看Hive数据格式</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -cat &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;mall.db&#x2F;interim_gender&#x2F;000000_0</span><br><span class="line">033232</span><br><span class="line">133215</span><br><span class="line">233553</span><br></pre></td></tr></table></figure><p><strong>数据由Tab键分割,改为：<code>--fields-terminated-by &#39;\t&#39;</code> 即可。</strong></p><h2 id="佛系更新……"><a href="#佛系更新……" class="headerlink" title="佛系更新……"></a>佛系更新……</h2>]]></content>
    
    <summary type="html">
    
      Sqoop遇到的问题
    
    </summary>
    
    
      <category term="Hadoop" scheme="https://smallbenxiong.github.io/categories/Hadoop/"/>
    
    
      <category term="Sqoop" scheme="https://smallbenxiong.github.io/tags/Sqoop/"/>
    
      <category term="Hadoop" scheme="https://smallbenxiong.github.io/tags/Hadoop/"/>
    
      <category term="BigData" scheme="https://smallbenxiong.github.io/tags/BigData/"/>
    
      <category term="大数据" scheme="https://smallbenxiong.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://smallbenxiong.github.io/2019/12/15/hello-world/"/>
    <id>https://smallbenxiong.github.io/2019/12/15/hello-world/</id>
    <published>2019-12-14T16:00:00.000Z</published>
    <updated>2020-01-06T14:32:48.301Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><ul><li><strong>这是我的个人博客，主要用于记录自己的学习笔记，未申请百度、谷歌等搜索功能，只能通过域名访问。😁😁😁</strong>   </li><li><strong>如果转载了您的文章，我会进行标注，主要是方便自己浏览和学习，如果您介意的话麻烦您留个言，我马上删除。🤞🙆‍♀️🙆‍♂️</strong>   </li><li><strong>如果您正好浏览到我的网页，那可真是太有缘了，留个言再走鸭鸭鸭。🙄🙄🙄</strong></li></ul><hr><h1 id="关于博客"><a href="#关于博客" class="headerlink" title="关于博客"></a>关于博客</h1><p>本<a href="https://smallbenxiong.github.io/">博客</a>是搭建在<a href="https://github.com/smallbenxiong" target="_blank" rel="noopener">GitHub</a>上使用的静态网页，网页使用了<a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>，这让我的博客搭建变得非常快捷，非常美观，而且完美适配手机端浏览。使用的主题为<a href="https://demo.jerryc.me/" target="_blank" rel="noopener">Butterfly</a>。<br>同时这也是一篇介绍功能使用和测试功能的文章，文章持续更新！</p><h1 id="博客使用方法"><a href="#博客使用方法" class="headerlink" title="博客使用方法"></a>博客使用方法</h1><h2 id="格式设置"><a href="#格式设置" class="headerlink" title="格式设置"></a>格式设置</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title:</span><br><span class="line">date:</span><br><span class="line">tags:</span><br><span class="line">categories:</span><br><span class="line">keywords:</span><br><span class="line">description:</span><br><span class="line">top_img: （除非特定需要，可以不写）</span><br><span class="line">comments  是否显示评论（除非设置false,可以不写）</span><br><span class="line">cover:  缩略图</span><br><span class="line">toc:  是否显示toc （除非特定文章设置，可以不写）</span><br><span class="line">toc_number: 是否显示toc数字 （除非特定文章设置，可以不写）</span><br><span class="line">copyright: 是否显示版权 （除非特定文章设置，可以不写）</span><br><span class="line">default_cover:</span><br><span class="line">top: </span><br><span class="line">---</span><br></pre></td></tr></table></figure><h3 id="不显示版权"><a href="#不显示版权" class="headerlink" title="不显示版权"></a>不显示版权</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果有文章（例如：转载文章）不需要显示版权，可以在文章Front-matter单独设置</span><br><span class="line">$ copyright: <span class="literal">false</span></span><br></pre></td></tr></table></figure><h3 id="文章封面"><a href="#文章封面" class="headerlink" title="文章封面"></a>文章封面</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">文章的markdown文档上,在Front-matter添加cover，并填上要显示的图片地址。</span><br><span class="line">当配置多张图片时，会随机选择一张作为cover。此时写法应为：</span><br><span class="line">default_cover:</span><br><span class="line">  - https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png</span><br><span class="line">  - https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png</span><br><span class="line">  - https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png</span><br></pre></td></tr></table></figure><h3 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在文章的front-matter区域里添加top: True属性来把这篇文章置顶。</span><br><span class="line">$ top: True</span><br></pre></td></tr></table></figure><h2 id="搜索系统"><a href="#搜索系统" class="headerlink" title="搜索系统"></a>搜索系统</h2><p>搜索系统使用<a href="https://www.algolia.com/" target="_blank" rel="noopener">Algolia</a>，配置参考<a href="https://github.com/oncletom/hexo-algolia" target="_blank" rel="noopener">hexo-algolia</a>。</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span>(windows为<span class="built_in">set</span>) HEXO_ALGOLIA_INDEXING_KEY=&#123;your key&#125;</span><br></pre></td></tr></table></figure><h3 id="更新Algolia库"><a href="#更新Algolia库" class="headerlink" title="更新Algolia库"></a>更新Algolia库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo algolia</span><br></pre></td></tr></table></figure><h2 id="音乐设置"><a href="#音乐设置" class="headerlink" title="音乐设置"></a>音乐设置</h2><p>使用<a href="https://github.com/MoePlayer/hexo-tag-aplayer" target="_blank" rel="noopener">hexo-tag-aplayer</a>的MeingJS插件，参考<a href="https://wiki.hushhw.cn/posts/a84d1ef1.html" target="_blank" rel="noopener">aplayer播放器配置</a> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 单首设置</span><br><span class="line">$ &#123;% meting <span class="string">"3986040"</span> <span class="string">"netease"</span> <span class="string">"song"</span> <span class="string">"theme:#555"</span> <span class="string">"mutex:true"</span> <span class="string">"listmaxheight:340px"</span> <span class="string">"preload:auto"</span> %&#125;</span><br><span class="line">2. 列表设置</span><br><span class="line">$ &#123;% meting <span class="string">"627070825"</span> <span class="string">"netease"</span> <span class="string">"playlist"</span> <span class="string">"theme:#555"</span> <span class="string">"mutex:true"</span> <span class="string">"listmaxheight:340px"</span> <span class="string">"preload:auto"</span> %&#125;</span><br></pre></td></tr></table></figure><h2 id="文件上传"><a href="#文件上传" class="headerlink" title="文件上传"></a>文件上传</h2><h3 id="生成静态文件"><a href="#生成静态文件" class="headerlink" title="生成静态文件"></a>生成静态文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure><h3 id="部署至github"><a href="#部署至github" class="headerlink" title="部署至github"></a>部署至github</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><h1 id="赞助"><a href="#赞助" class="headerlink" title="赞助"></a>赞助</h1><table><thead><tr><th align="center">用户名</th><th align="center">金额/元</th><th align="center">留言</th></tr></thead><tbody><tr><td align="center">Ben</td><td align="center">99</td><td align="center">Thanks！</td></tr><tr><td align="center">…</td><td align="center">…</td><td align="center">…</td></tr></tbody></table><p>More info: <a href="https://smallbenxiong.github.io/">Ben Blog</a></p>]]></content>
    
    <summary type="html">
    
      博客的使用方法
    
    </summary>
    
    
      <category term="Blog" scheme="https://smallbenxiong.github.io/categories/Blog/"/>
    
    
      <category term="Blog" scheme="https://smallbenxiong.github.io/tags/Blog/"/>
    
  </entry>
  
</feed>
